# ğŸ–ï¸ Sign Speak

## ğŸ“Œ Overview  
**Sign Speak** is a real-time tool that translates sign language gestures into text using **computer vision**, **MediaPipe**, and **machine learning**. It helps bridge communication gaps for individuals with hearing impairments.  

## ğŸ¯ Features  
- âœ… **Real-time Sign Language Detection** using a webcam  
- âœ… **Hand Gesture Tracking** with MediaPipe  
- âœ… **Machine Learning-based Gesture Classification**  
- âœ… **User-friendly Chat Interface** for communication  
- âœ… **Speech-to-Text Integration** (for nurses/assistants)  
- âœ… **Role-based UI** (Patient & Nurse modes)  

## ğŸ—ï¸ Project Structure  

```plaintext
ğŸ“‚ SignLanguageTranslator/
â”‚â”€â”€ ğŸ“œ README.md                # Documentation  
â”‚â”€â”€ ğŸ“‚ models/                  # Trained ML models  
â”‚â”€â”€ ğŸ“‚ data/                    # Gesture datasets  
â”‚â”€â”€ ğŸ“œ data_collection.py       # Captures gesture data  
â”‚â”€â”€ ğŸ“œ train_model.py           # Trains the ML model  
â”‚â”€â”€ ğŸ“œ realtime_prediction.py   # Real-time recognition  
â”‚â”€â”€ ğŸ“œ mp_utils.py              # MediaPipe utilities  
â”‚â”€â”€ ğŸ“œ main.py                  # Main execution file  
â”‚â”€â”€ ğŸ“‚ static/                  # CSS & JavaScript files  
â”‚â”€â”€ ğŸ“‚ templates/               # Web HTML templates  
â”‚â”€â”€ ğŸ“œ requirements.txt         # Dependencies  

âš™ï¸ Installation & Setup

1ï¸âƒ£ Clone the Repository

git clone https://github.com/your-username/SignLanguageTranslator.git
cd SignLanguageTranslator

2ï¸âƒ£ Install Dependencies

Ensure Python 3.8+ is installed, then run:

pip install -r requirements.txt

3ï¸âƒ£ Run the Application

python main.py

ğŸš€ Usage

1ï¸âƒ£ Select a Role: Choose between Nurse (Text/Voice) or Patient (Sign Language).
2ï¸âƒ£ For Patients: Activate Gesture Detection to recognize signs.
3ï¸âƒ£ For Nurses: Use text input or voice-to-text for communication.
4ï¸âƒ£ Chat Window: Messages appear in real-time for seamless interaction.

ğŸ› ï¸ Technical Stack
	â€¢	Python (Core programming)
	â€¢	OpenCV (Video processing)
	â€¢	MediaPipe (Hand gesture tracking)
	â€¢	TensorFlow/Keras (Model training)
	â€¢	Flask (Web-based interface)
	â€¢	JavaScript (Web Speech API) (Voice-to-text support)

ğŸ“Š Model Training

# Collect gesture data
python data_collection.py  

# Train the model
python train_model.py  

# Run real-time gesture detection
python realtime_prediction.py  


ğŸ¤ Contributing
	1.	Fork the repository
	2.	Create a new branch: git checkout -b feature-name
	3.	Commit changes: git commit -m "Added feature"
	4.	Push to branch: git push origin feature-name
	5.	Open a Pull Request

ğŸ“œ License

This project is licensed under the MIT License.

